{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 285 Assignment 1: Logistic Regression\n",
    "\n",
    "For this part of assignment, you are tasked to implement a logistic regression algorithm for multiclass classification and test it on the CIFAR10 dataset.\n",
    "\n",
    "You sould run the whole notebook and answer the questions in the notebook.\n",
    "\n",
    "TO SUBMIT: PDF of this notebook with all the required outputs and answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test'])\n",
      "Training Set Data  Shape:  (5000, 3072)\n",
      "Training Set Label Shape:  (5000,)\n",
      "Validation Set Data  Shape:  (250, 3072)\n",
      "Validation Set Label Shape:  (250,)\n",
      "Test Set Data  Shape:  (500, 3072)\n",
      "Test Set Label Shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ece285.utils.data_processing import get_cifar10_data\n",
    "\n",
    "# Use a subset of CIFAR10 for KNN assignments\n",
    "dataset = get_cifar10_data(\n",
    "    subset_train=5000,\n",
    "    subset_val=250,\n",
    "    subset_test=500,\n",
    ")\n",
    "\n",
    "print(dataset.keys())\n",
    "print(\"Training Set Data  Shape: \", dataset[\"x_train\"].shape)\n",
    "print(\"Training Set Label Shape: \", dataset[\"y_train\"].shape)\n",
    "print(\"Validation Set Data  Shape: \", dataset[\"x_val\"].shape)\n",
    "print(\"Validation Set Label Shape: \", dataset[\"y_val\"].shape)\n",
    "print(\"Test Set Data  Shape: \", dataset[\"x_test\"].shape)\n",
    "print(\"Test Set Label Shape: \", dataset[\"y_test\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for multi-class classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Logistic Regression Algorithm has 3 hyperparameters that you can experiment with:\n",
    "\n",
    "- **Learning rate** - controls how much we change the current weights of the classifier during each update. We set it at a default value of 0.5, and later you are asked to experiment with different values. We recommend looking at the graphs and observing how the performance of the classifier changes with different learning rate.\n",
    "- **Number of Epochs** - An epoch is a complete iterative pass over all of the data in the dataset. During an epoch we predict a label using the classifier and then update the weights of the classifier according the linear classifier update rule for each sample in the training set. We evaluate our models after every 10 epochs and save the accuracies, which are later used to plot the training, validation and test VS epoch curves.\n",
    "- **Weight Decay** - Regularization can be used to constrain the weights of the classifier and prevent their values from blowing up. Regularization helps in combatting overfitting. You will be using the 'weight_decay' term to introduce regularization in the classifier.\n",
    "\n",
    "The only way how a Logistic Regression based classification algorithm is different from a Linear Regression algorithm is that in the former we additionally pass the classifier outputs into a sigmoid function which squashes the output in the (0,1) range. Essentially these values then represent the probabilities of that sample belonging to class particular classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation (40%)\n",
    "\n",
    "You need to implement the Linear Regression method in `algorithms/logistic_regression.py`. You need to fill in the sigmoid function, training function as well as the prediction function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the algorithm implementation (TODO: Complete the Logistic Regression in algorithms/logistic_regression.py)\n",
    "from ece285.algorithms import Logistic\n",
    "from ece285.utils.evaluation import get_classification_accuracy\n",
    "\n",
    "num_classes = 10  # Cifar10 dataset has 10 different classes\n",
    "\n",
    "# Initialize hyper-parameters\n",
    "learning_rate = 0.01  # You will be later asked to experiment with different learning rates and report results\n",
    "num_epochs_total = 1000  # Total number of epochs to train the classifier\n",
    "epochs_per_evaluation = 10  # Epochs per step of evaluation; We will evaluate our model regularly during training\n",
    "N, D = dataset[\n",
    "    \"x_train\"\n",
    "].shape  # Get training data shape, N: Number of examples, D:Dimensionality of the data\n",
    "weight_decay = 0.00002\n",
    "\n",
    "x_train = dataset[\"x_train\"].copy()\n",
    "y_train = dataset[\"y_train\"].copy()\n",
    "x_val = dataset[\"x_val\"].copy()\n",
    "y_val = dataset[\"y_val\"].copy()\n",
    "x_test = dataset[\"x_test\"].copy()\n",
    "y_test = dataset[\"y_test\"].copy()\n",
    "\n",
    "# Insert additional scalar term 1 in the samples to account for the bias as discussed in class\n",
    "x_train = np.insert(x_train, D, values=1, axis=1)\n",
    "x_val = np.insert(x_val, D, values=1, axis=1)\n",
    "x_test = np.insert(x_test, D, values=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation function -> Outputs accuracy data\n",
    "def train(learning_rate_, weight_decay_):\n",
    "    # Create a linear regression object\n",
    "    logistic_regression = Logistic(\n",
    "        num_classes, learning_rate_, epochs_per_evaluation, weight_decay_\n",
    "    )\n",
    "\n",
    "    # Randomly initialize the weights and biases\n",
    "    weights = np.random.randn(num_classes, D + 1) * 0.0001\n",
    "\n",
    "    train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "\n",
    "    # Train the classifier\n",
    "    for _ in tqdm(range(int(num_epochs_total / epochs_per_evaluation))):\n",
    "        # Train the classifier on the training data\n",
    "        weights = logistic_regression.train(x_train, y_train, weights)\n",
    "\n",
    "        # Evaluate the trained classifier on the training dataset\n",
    "        y_pred_train = logistic_regression.predict(x_train)\n",
    "        train_accuracies.append(get_classification_accuracy(y_pred_train, y_train))\n",
    "\n",
    "        # Evaluate the trained classifier on the validation dataset\n",
    "        y_pred_val = logistic_regression.predict(x_val)\n",
    "        val_accuracies.append(get_classification_accuracy(y_pred_val, y_val))\n",
    "\n",
    "        # Evaluate the trained classifier on the test dataset\n",
    "        y_pred_test = logistic_regression.predict(x_test)\n",
    "        test_accuracies.append(get_classification_accuracy(y_pred_test, y_test))\n",
    "\n",
    "    return train_accuracies, val_accuracies, test_accuracies, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_accuracies(train_acc, val_acc, test_acc):\n",
    "    # Plot Accuracies vs Epochs graph for all the three\n",
    "    epochs = np.arange(0, int(num_epochs_total / epochs_per_evaluation))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch/10\")\n",
    "    plt.plot(epochs, train_acc, epochs, val_acc, epochs, test_acc)\n",
    "    plt.legend([\"Training\", \"Validation\", \"Testing\"])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and plotting for default parameter values as mentioned above\n",
    "t_ac, v_ac, te_ac, weights = train(learning_rate, weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(t_ac, v_ac, te_ac)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different learning rates and plot graphs for all (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best values\n",
    "best_weights = weights\n",
    "best_learning_rate = learning_rate\n",
    "best_weight_decay = weight_decay\n",
    "\n",
    "# TODO\n",
    "# Repeat the above training and evaluation steps for the following learning rates and plot graphs\n",
    "# You need to try 3 learning rates and submit all 3 graphs along with this notebook pdf to show your learning rate experiments\n",
    "learning_rates = []\n",
    "weight_decay = 0.0  # No regularization for now\n",
    "\n",
    "# FEEL FREE TO EXPERIMENT WITH OTHER VALUES. REPORT OTHER VALUES IF THEY ACHIEVE A BETTER PERFORMANCE\n",
    "\n",
    "# for lr in learning_rates: Train the classifier and plot data\n",
    "# Step 1. train_accu, val_accu, test_accu = train(lr, weight_decay)\n",
    "# Step 2. plot_accuracies(train_accu, val_accu, test_accu)\n",
    "\n",
    "learning_rate_info = {}\n",
    "best_learning_rate\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    # TODO: Train the classifier with different learning rates and plot\n",
    "    # pass\n",
    "    t_ac, v_ac, te_ac, weights = train(learning_rate, weight_decay)\n",
    "    learning_rate_info[learning_rate] = [t_ac, v_ac, te_ac, weights]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, learning_rate in enumerate(learning_rates):\n",
    "    t_ac, v_ac, te_ac = learning_rate_info[learning_rate][0], learning_rate_info[learning_rate][1], learning_rate_info[learning_rate][2]\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.title(\"Learning rate: {}\".format(learning_rate))\n",
    "    plot_accuracies(t_ac, v_ac, te_ac)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question 1.\n",
    "\n",
    "Which one of these learning rates (best_lr) would you pick to train your model? Please Explain why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Try different weight decay and plots graphs for all (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a non-zero weight_decay (Regulzarization constant) term and repeat the training and evaluation\n",
    "# Use the best learning rate as obtained from the above excercise, best_lr\n",
    "\n",
    "# You need to try 3 learning rates and submit all 3 graphs along with this notebook pdf to show your weight decay experiments\n",
    "weight_decays = []\n",
    "\n",
    "# FEEL FREE TO EXPERIMENT WITH OTHER VALUES. REPORT OTHER VALUES IF THEY ACHIEVE A BETTER PERFORMANCE\n",
    "\n",
    "# for weight_decay in weight_decays: Train the classifier and plot data\n",
    "# Step 1. train_accu, val_accu, test_accu = train(best_lr, weight_decay)\n",
    "# Step 2. plot_accuracies(train_accu, val_accu, test_accu)\n",
    "\n",
    "for weight_decay in weight_decays:\n",
    "    # TODO: Train the classifier with different weighty decay and plot\n",
    "    # pass\n",
    "    t_ac, v_ac, te_ac, weights = train(best_learning_rate, weight_decay)\n",
    "    weight_decay_info[weight_decay] = [t_ac, v_ac, te_ac, weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question 2.\n",
    "\n",
    "Discuss underfitting and overfitting as observed in the 5 graphs obtained by changing the regularization.\n",
    "Which weight_decay term gave you the best classifier performance?\n",
    "HINT: Do not just think in terms of best training set performance, keep in mind that the real utility of a machine learning model is when it performs well on data it has never seen before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the filters (10%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These visualizations will only somewhat make sense if your learning rate and weight_decay parameters were\n",
    "# properly chosen in the model. Do your best.\n",
    "\n",
    "# TODO: Run this cell and Show filter visualizations for the best set of weights you obtain.\n",
    "# Report the 2 hyperparameters you used to obtain the best model.\n",
    "\n",
    "# NOTE: You need to set `best_learning_rate` and `best_weight_decay` to the values that gave the highest accuracy\n",
    "print(\"Best LR:\", best_learning_rate)\n",
    "print(\"Best Weight Decay:\", best_weight_decay)\n",
    "\n",
    "# NOTE: You need to set `best_weights` to the weights with the highest accuracy\n",
    "w = best_weights[:, :-1]\n",
    "w = w.reshape(10, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "classes = [\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "for i in range(10):\n",
    "    fig.add_subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[i, :, :, :].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype(int))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(classes[i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question 3. (10%)\n",
    "\n",
    "a. Compare and contrast the performance of the 2 classifiers i.e. Linear Regression and Logistic Regression.\n",
    "b. Which classifier would you deploy for your multiclass classification project and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
